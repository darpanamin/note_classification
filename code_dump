Cluster 3


hdfs dfs -copyFromLocal HP_NOTE_TEXT_TRAIN_YES.csv

pyspark2

df = sqlContext.read.format("csv").options(header='true', inferschema='true').option("delimiter", "|").load("hdfs://rushdatascience/user/damin/HP_NOTE_TEXT_TRAIN_YES.csv")
df.show()
df.printSchema()

 EPIC_PAT_ID: string (nullable = true)
 |-- MRN: integer (nullable = true)
 |-- SURG_DATE: integer (nullable = true)
 |-- ALL_ELEMENTS_ADDED_YN: string (nullable = true)
 |-- NOTE_ID: integer (nullable = true)
 |-- NOTE_CSN_ID: integer (nullable = true)
 |-- LINE: integer (nullable = true)
 |-- NOTE_TEXT: string (nullable = true)


    /**** common steps ******/
df.createOrReplaceTempView('train_data')

result = sqlContext.sql("select EPIC_PAT_ID,MRN,NOTE_ID,NOTE_CSN_ID,SURG_DATE,ALL_ELEMENTS_ADDED_YN,concat_ws('\n ', collect_list(NOTE_TEXT)) as MERGED_NOTE_TEXT from train_data group by EPIC_PAT_ID,MRN,NOTE_ID,NOTE_CSN_ID,SURG_DATE,ALL_ELEMENTS_ADDED_YN")
result.count()
    /***** /
    
result.write.parquet("HP_NOTE_TEXT_MERGED_YES.parquet")

 /*** load rejected notes *****/
df = sqlContext.read.format("csv").options(header='true', inferschema='true').option("delimiter", "|").load("hdfs://rushdatascience/user/damin/HP_NOTE_TEXT_TRAIN_NO.csv")

result.write.parquet("HP_NOTE_TEXT_MERGED_NO.parquet")


parquet_HP_yes = spark.read.parquet("hdfs://rushdatascience/user/damin/HP_NOTE_TEXT_MERGED_YES.parquet")
parquet_HP_no = spark.read.parquet("hdfs://rushdatascience/user/damin/HP_NOTE_TEXT_MERGED_NO.parquet")
# returns dataframe
 t10 = parquet_HP_yes.limit(10)

# returns list
 t10 = parquet_HP_yes.take(10)

limit_train = parquet_HP_yes.selectExpr("NOTE_ID","ALL_ELEMENTS_ADDED_YN as TARGET_CLASS","MERGED_NOTE_TEXT").limit(100)
full_train = parquet_HP_yes.selectExpr("NOTE_ID","ALL_ELEMENTS_ADDED_YN as TARGET_CLASS","MERGED_NOTE_TEXT")

limit_train_no = parquet_HP_no.selectExpr("NOTE_ID","ALL_ELEMENTS_ADDED_YN as TARGET_CLASS","MERGED_NOTE_TEXT").limit(15)
full_train_no = parquet_HP_no.selectExpr("NOTE_ID","ALL_ELEMENTS_ADDED_YN as TARGET_CLASS","MERGED_NOTE_TEXT")


import random 
group_of_items = {1, 2, 3, 4} # a sequence or set will work here. 
num_to_select = 2 # set the number to select here. 
list_of_random_items = random.sample(group_of_items, num_to_select)


df = dfRawData.where(col("X").isin({"CB", "CI", "CR"}))

/*** merged dataset *****/
 limit_train = limit_train.union(limit_train_no)
full_train=full_train.union(full_train_no)

/*** create sample for test ******/
sample_set = parquet_HP_no.sample(False, .56)
sample_set = parquet_HP_no.sample(False, .56, seed = 0)

limit_test = full_train.sample(False, .56, seed = 0)
from pyspark.sql.functions import regexp_extract, col
df.withColumn('Employee', regexp_extract(col('Notes'), '(.)(by)(\s+)(\w+)', 4))
df.select(regexp_extract('str', '(\d+)-(\d+)', 1).alias('d')).collect()



/*** create pandas ***/
 train_pd = limit_train.toPandas()

test_pd = limit_test.toPandas()

train_notes = train_pd.ix[:,2].tolist()
train_target = train_pd.ix[:,1].tolist()

test_notes = test_pd.ix[:,2].tolist()
test_target = test_pd.ix[:,1].tolist()

 # review the test vector
 test_target.count("N")

vect = CountVectorizer()
# this was better on false positives
vect = CountVectorizer(max_df=0.95, min_df=2,stop_words='english')

vect = TfidfVectorizer()

# this one is cool too
vect = TfidfVectorizer(max_df=0.95, min_df=2,stop_words='english')

 #get the dtm vectors
dtm = vect.fit_transform(train_notes)
dtm_test = vect.transform(test_notes)

ch2 = SelectKBest(chi2, k=25)

vocab = vect.vocabulary_
vectpd = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())

vectpd.ix[:, ['chief','complaint']]

from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.naive_bayes import BernoulliNB
from sklearn.feature_selection import SelectKBest, chi2
 X_train = ch2.fit_transform(dtm, train_target)


 # this one sucks
 clf = MultinomialNB().fit(dtm, train_target)

# this is better
clf = BernoulliNB()
clf = clf.fit(dtm, train_target)
print(clf.predict(dtm[1:12]))

 g= clf.predict(dtm[:114])
(g == 'Y').sum()
 (g == 'N').sum()

pred_results=clf.predict(dtm[0:115])

metrics.accuracy_score(train_target, g)
metrics.confusion_matrix(train_target, g)
clf.predict_proba(dtm)[:,1]



myresults=pd.DataFrame( {'Note_ID': train_pd.ix[:,0].tolist(), 'NOTE_STATUS_ACTUAL': train_target, 'PREDICTED_STATUS': pred_results })



/*** scikit learn *******/
print("\n".join(twenty_train.data[0].split("\n")[:7]))

import re
import string



re.sub(r'[^a-zA-Z ]', '', twenty_train.data[0])

print("\n".join(twenty_train.data[0].lower().split(" ")[:7]))

print("\n".join((re.sub(r'[^a-zA-Z ]', '', twenty_train.data[0])) .lower().split(" ")[:7]))

 /*** this works better ***/
wordList =  re.sub("[^\w]"," ", twenty_train.data[0]).lower().split()

wordList =  re.sub(r'[^a-zA-Z ]', '', twenty_train.data[0]).lower().split()

re.sub('[0-9]','',"Hello r2d2").split()
['Hello', 'rd']
re.sub('[0-9]',' ',"Hello r2d2").split()
['Hello', 'r', 'd']

wordList = re.sub('[0-9]',' ', re.sub("[^\w]"," ", twenty_train.data[0])).lower().split()

import nltk




import re 
shop="hello seattle what have you got" 
regex = r'\w+' 
regex = r'at|yo'
list1=re.findall(regex,shop)
from string import upper
From string import lower
From string import join
mylis=['this is test', 'another test'] 
map(upper, mylis)
print map(lambda x: x * 2 + 10, foo)
map(lambda x: re.findall(regex,x),mylis)
map(lambda x: " ".join(x),yo)




samp_list =  parquet_HP.select("MERGED_NOTE_TEXT").collect()
flat_list = [item for sublist in samp_list for item in sublist]


X_train_counts = count_vect.fit_transform(flat_list)
X_train_counts.shape



/*** getting a DTM ***/
 count_vect.get_feature_names()

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()
vect = CountVectorizer(max_df=0.95, min_df=2,stop_words='english')

vect.fit(flat_list)
vect.get_feature_names()

dtm = vect.transform(flat_fit)
import pandas as pd
pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())

from sklearn.naive_bayes import BernoulliNB
import numpy as np
X = np.random.randint(2, size=(6, 100))
Y = np.array([1, 2, 3, 4, 4, 5])
Y = np.array(['A', 'N', 'K','P', 'P', 'Y'])

from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()
clf.fit(X, Y)

print(clf.predict(X[2:3]))


Drop column in pandas
df = df.drop('column_name', 1)


from sklearn.feature_selection import chi2


# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)
import pandas
import numpy
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
# load data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
# feature extraction
test = SelectKBest(score_func=chi2, k=4)
fit = test.fit(X, Y)
# summarize scores
numpy.set_printoptions(precision=3)
print(fit.scores_)
features = fit.transform(X)
# summarize selected features
print(features[0:5,:])
print("Selected Features: %s") % fit.get_support()


# to filter out useless terms early on: the posts are stripped of headers,
# footers and quoted replies, and common English words, words occurring in
# only one document or in at least 95% of the documents are removed.

print("Extracting tf-idf features for NMF...")
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,
                                   max_features=n_features,
                                   stop_words='english')
